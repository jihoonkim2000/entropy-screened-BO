{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "faad807d-f68b-40d8-b519-5e017ccd9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2457ad44-ef24-41f8-9875-7e877c19e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Configuration -----------------------------\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_PROJS = 50\n",
    "DATA_DIR = 'directory_to_images'\n",
    "PROJ_SIZE = 512    # original projection resolution\n",
    "REC_SIZE = 512      # downsampled for reconstruction\n",
    "OUT_SIZE = 512    # output video frame resolution\n",
    "ANGLES = torch.linspace(-72, 75, steps=NUM_PROJS).mul(math.pi/180).to(DEVICE)\n",
    "\n",
    "# Define rotation axis (perpendicular to view). Example: camera looks down -Z, so axis=X or Y works.\n",
    "# Specify as a 3-element tuple, e.g., axis = (1,0,0) for X, (0,1,0) for Y, (0,0,1) for Z,\n",
    "# or any arbitrary axis.\n",
    "ROT_AXIS = torch.tensor((0,1,0), dtype=torch.float32, device=DEVICE)  # change as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "62aa7a83-4f36-4836-a5fb-7af4ef1227cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Load & Downsample Projections (RGB) -------\n",
    "files = [os.path.join(DATA_DIR, f\"overlay_{i}.png\") for i in range(1, NUM_PROJS+1)]\n",
    "projs = []\n",
    "for fp in files:\n",
    "    img = Image.open(fp).convert('RGB').resize((REC_SIZE, REC_SIZE), Image.BILINEAR)\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0  # normalize\n",
    "    tensor = torch.from_numpy(arr).permute(2,0,1)\n",
    "    projs.append(tensor)\n",
    "# projs shape: (NUM_PROJS, 3, REC_SIZE, REC_SIZE)\n",
    "projs = torch.stack(projs, dim=0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "da32fcab-cec0-4ad0-a496-d02791026f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Ramp Filter --------------------------------\n",
    "def ramp_filter_1d(signal):\n",
    "    \"\"\"Apply ramp filter to 1D signal of shape (W,)\"\"\"\n",
    "    N = signal.shape[-1]\n",
    "    freqs = torch.fft.rfftfreq(N, d=1.0).to(DEVICE)\n",
    "    filt = freqs.abs()\n",
    "    P = torch.fft.rfft(signal, dim=-1)\n",
    "    return torch.fft.irfft(P * filt, n=N, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a78df32e-5799-4147-ab95-7e4174414aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Backprojection -----------------------------\n",
    "def backproject_slice(z_idx):\n",
    "    # sinogram: (NUM_PROJS, 3, REC_SIZE)\n",
    "    sinogram = projs[:, :, z_idx, :]\n",
    "    W = REC_SIZE\n",
    "    # Create backprojection grid coordinates\n",
    "    coords = torch.linspace(-(W-1)/2, (W-1)/2, W, device=DEVICE)\n",
    "    X, Y = torch.meshgrid(coords, coords, indexing='xy')\n",
    "    recon = torch.zeros((3, W, W), device=DEVICE)\n",
    "    for i, theta in enumerate(ANGLES):\n",
    "        cos_t, sin_t = math.cos(theta), math.sin(theta)\n",
    "        for c in range(3):\n",
    "            proj_line = sinogram[i, c, :]          # (W,)\n",
    "            filtered = ramp_filter_1d(proj_line)   # (W,)\n",
    "            # compute sampling grid of shape (1, W, W, 2)\n",
    "            t = X * cos_t + Y * sin_t\n",
    "            idx_norm = (t + (W-1)/2) * 2/(W-1) - 1  # in [-1,1]\n",
    "            # first coord (y) is zeros since proj_line is 1D along width\n",
    "            zeros = torch.zeros_like(idx_norm)\n",
    "            grid = torch.stack([zeros, idx_norm], dim=-1).unsqueeze(0)  # (1, W, W, 2)\n",
    "            # sample filtered projection into 2D slice\n",
    "            sampled = F.grid_sample(filtered.view(1,1,1,W), grid, align_corners=False)\n",
    "            # sampled: (1,1,W,W?) -> view as (W, W)\n",
    "            recon[c] += sampled.view(W, W)\n",
    "    recon *= (math.pi / (2 * ANGLES.numel()))\n",
    "    return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "59273d6f-678c-4ec2-a983-4c1a0b5241c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Reconstruct Volume ------------------------. Reconstruct Volume ------------------------\n",
    "vol_slices = [backproject_slice(z) for z in range(REC_SIZE)]\n",
    "vol_stack = torch.stack(vol_slices, dim=0)  # (Z,3,Y,X)\n",
    "volume = vol_stack.permute(1,0,2,3).unsqueeze(0)  # (1,3,Z,Y,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0c16179d-8c81-4b9b-9613-a2d6fe4cf83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. 3D MIP Rendering on Arbitrary Axis --------\n",
    "def render_mip_axis(vol5d, yaw, axis_vec):\n",
    "    a = axis_vec / axis_vec.norm()\n",
    "    ux, uy, uz = a\n",
    "    c, s = math.cos(yaw), math.sin(yaw)\n",
    "    R = torch.tensor([\n",
    "        [c+ux*ux*(1-c), ux*uy*(1-c)-uz*s, ux*uz*(1-c)+uy*s],\n",
    "        [uy*ux*(1-c)+uz*s, c+uy*uy*(1-c), uy*uz*(1-c)-ux*s],\n",
    "        [uz*ux*(1-c)-uy*s, uz*uy*(1-c)+ux*s, c+uz*uz*(1-c)]\n",
    "    ], device=DEVICE)\n",
    "    _, C, Z, Y, X = vol5d.shape\n",
    "    zs = torch.linspace(-1,1,Z,device=DEVICE)\n",
    "    ys = torch.linspace(-1,1,Y,device=DEVICE)\n",
    "    xs = torch.linspace(-1,1,X,device=DEVICE)\n",
    "    zz, yy, xx = torch.meshgrid(zs, ys, xs, indexing='ij')\n",
    "    coords = torch.stack([zz, yy, xx], dim=-1)\n",
    "    rot = (coords.view(-1,3) @ R.T).view(Z,Y,X,3)\n",
    "    grid = rot.view(1,Z,Y,X,3)\n",
    "    sampled = F.grid_sample(vol5d, grid, mode='bilinear', align_corners=True)\n",
    "    mip = sampled.amax(dim=2)  # (1,C,Y,X)\n",
    "    return mip[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "348d764c-f60f-4488-b5b9-f469e24bcdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Save Frames & Upsample Output ------------\n",
    "n_frames = 180\n",
    "os.makedirs('frames', exist_ok=True)\n",
    "for i in range(n_frames):\n",
    "    angle = 2*math.pi * i / n_frames\n",
    "    mip = render_mip_axis(volume, angle, ROT_AXIS).cpu().numpy()  # (3,Y,X)\n",
    "    img_accum = np.zeros_like(mip)\n",
    "    for c in range(3):\n",
    "        chan = mip[c]\n",
    "        p1, p99 = np.percentile(chan, (1,99))\n",
    "        img_accum[c] = np.clip((chan-p1)/(p99-p1),0,1) if p99>p1 else (chan-chan.min())/(chan.max()-chan.min()+1e-6)\n",
    "    img = (np.clip(img_accum,0,1).transpose(1,2,0) * 255).astype('uint8')\n",
    "    Image.fromarray(img).resize((OUT_SIZE,OUT_SIZE), Image.BILINEAR).save(f'frames/frame_{i:03d}.png')\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba51f739-e559-4db1-b33c-06049b20ada2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_260",
   "language": "python",
   "name": "pytorch_260"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
