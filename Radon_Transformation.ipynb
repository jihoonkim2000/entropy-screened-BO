{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faad807d-f68b-40d8-b519-5e017ccd9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2457ad44-ef24-41f8-9875-7e877c19e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Configuration -----------------------------\n",
    "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "DATA_DIR = './images_4k'\n",
    "NUM_PROJS = 50\n",
    "PROJ_SIZE = 4096    # original projection resolution\n",
    "REC_SIZE = 1024      # downsampled for reconstruction\n",
    "OUT_SIZE = 1024     # output video frame resolution\n",
    "ANGLES = torch.linspace(-72, 75, steps=NUM_PROJS).mul(math.pi/180).to(DEVICE)\n",
    "\n",
    "# Define rotation axis (perpendicular to view). Example: camera looks down -Z, so axis=X or Y works.\n",
    "# Specify as a 3-element tuple, e.g., axis = (1,0,0) for X, (0,1,0) for Y, (0,0,1) for Z,\n",
    "# or any arbitrary axis.\n",
    "ROT_AXIS = torch.tensor((0,1,0), dtype=torch.float32, device=DEVICE)  # change as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62aa7a83-4f36-4836-a5fb-7af4ef1227cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Load & Downsample Projections -------------\n",
    "files = [os.path.join(DATA_DIR, f\"{i}.png\") for i in range(1, NUM_PROJS+1)]\n",
    "projs = []\n",
    "for fp in files:\n",
    "    img = Image.open(fp).convert('L').resize((REC_SIZE, REC_SIZE), Image.BILINEAR)\n",
    "    arr = np.array(img, dtype=np.float32)\n",
    "    projs.append(torch.from_numpy(arr))\n",
    "projs = torch.stack(projs, dim=0).to(DEVICE)  # (N, REC_SIZE, REC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da32fcab-cec0-4ad0-a496-d02791026f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Ramp Filter --------------------------------\n",
    "def ramp_filter(proj):\n",
    "    N = proj.shape[-1]\n",
    "    freqs = torch.fft.rfftfreq(N, d=1.0).to(DEVICE)\n",
    "    filt = freqs.abs()\n",
    "    P = torch.fft.rfft(proj, dim=-1)\n",
    "    P = P * filt.unsqueeze(0)\n",
    "    return torch.fft.irfft(P, n=N, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a78df32e-5799-4147-ab95-7e4174414aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Backprojection -----------------------------\n",
    "def backproject_slice(z_idx, projs, angles):\n",
    "    sinogram = projs[:, z_idx, :]\n",
    "    filt = ramp_filter(sinogram)\n",
    "    W = filt.shape[-1]\n",
    "    coords = torch.linspace(-(W-1)/2, (W-1)/2, W, device=DEVICE)\n",
    "    X, Y = torch.meshgrid(coords, coords, indexing='xy')\n",
    "    recon = torch.zeros((W, W), device=DEVICE)\n",
    "    for theta, proj_f in zip(angles, filt):\n",
    "        t = X * torch.cos(theta) + Y * torch.sin(theta)\n",
    "        idx_norm = (t + (W-1)/2) * 2/(W-1) - 1\n",
    "        grid = torch.stack([idx_norm, torch.zeros_like(idx_norm)], dim=-1).unsqueeze(0)\n",
    "        sampled = F.grid_sample(proj_f.view(1,1,1,W), grid, align_corners=False)\n",
    "        recon += sampled.view(W, W)\n",
    "    return recon * (math.pi / (2 * angles.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59273d6f-678c-4ec2-a983-4c1a0b5241c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Reconstruct Volume ------------------------\n",
    "volume = torch.stack([backproject_slice(z, projs, ANGLES) for z in range(REC_SIZE)], dim=0)  # (Z, Y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c16179d-8c81-4b9b-9613-a2d6fe4cf83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Generic MIP Rendering ----------------------\n",
    "def render_mip_axis(vol, yaw, axis_vec):\n",
    "    \"\"\"\n",
    "    Rotate volume around arbitrary axis_vec by yaw (radians), then max-intensity-project along camera direction (-Z).\n",
    "    Assumes camera looks along -Z, volume coords in (Z,Y,X).\n",
    "    \"\"\"\n",
    "    # Normalize axis\n",
    "    a = axis_vec / axis_vec.norm()\n",
    "    ux, uy, uz = a\n",
    "    # Rodrigues rotation matrix components\n",
    "    c, s = math.cos(yaw), math.sin(yaw)\n",
    "    R = torch.tensor([\n",
    "        [c + ux*ux*(1-c),    ux*uy*(1-c) - uz*s, ux*uz*(1-c) + uy*s],\n",
    "        [uy*ux*(1-c) + uz*s, c + uy*uy*(1-c),    uy*uz*(1-c) - ux*s],\n",
    "        [uz*ux*(1-c) - uy*s, uz*uy*(1-c) + ux*s, c + uz*uz*(1-c)]\n",
    "    ], device=DEVICE)\n",
    "\n",
    "    Z, Y, X = vol.shape\n",
    "    # Create meshgrid coords in normalized [-1,1]\n",
    "    zs = torch.linspace(-1,1,Z,device=DEVICE)\n",
    "    ys = torch.linspace(-1,1,Y,device=DEVICE)\n",
    "    xs = torch.linspace(-1,1,X,device=DEVICE)\n",
    "    zz, yy, xx = torch.meshgrid(zs, ys, xs, indexing='ij')  # (Z,Y,X)\n",
    "    coords = torch.stack([zz, yy, xx], dim=-1)  # (Z,Y,X,3)\n",
    "\n",
    "    # Apply rotation\n",
    "    coords_flat = coords.view(-1, 3)  # (N,3)\n",
    "    rot_flat = coords_flat @ R.T\n",
    "    rot = rot_flat.view(Z, Y, X, 3)\n",
    "\n",
    "    # Map back to sampling grid for camera projecting along -Z: sample rotated vol\n",
    "    # We need XY plane sampling: for each (y,x), sample along Z dimension max\n",
    "    # Build grid for grid_sample: samples each Z slice into image\n",
    "    # Permute vol to (1,1,Z,Y,X) for 3D sampling is complexâ€”approximate by sampling 2D slices\n",
    "    # We'll sample each Z slice: for each z index, project rotated volume's corresponding intensity\n",
    "    # Instead use nearest neighbor: find for each ray along Z the max intensity\n",
    "    # Simplify: rotate points, then MIP: gather vol at rotated coords using trilinear\n",
    "    # Use grid_sample 3D\n",
    "    vol_5d = vol.unsqueeze(0).unsqueeze(0)  # (1,1,Z,Y,X)\n",
    "    grid_3d = rot.view(1, Z, Y, X, 3)\n",
    "    # grid_sample expects coords in [-1,1] as (x,y,z)\n",
    "    # swap to (N,C,D,H,W) and grid (N,D,H,W,3)\n",
    "    sampled = F.grid_sample(vol_5d, grid_3d, mode='bilinear', align_corners=True)\n",
    "    # sampled shape: (1,1,Z,Y,X)\n",
    "    sampled = sampled[0,0]  # (Z,Y,X)\n",
    "    # MIP along Z axis\n",
    "    mip = sampled.amax(dim=0)  # (Y,X)\n",
    "    return mip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "348d764c-f60f-4488-b5b9-f469e24bcdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Save Frames -------------------------------\n",
    "n_frames = 180\n",
    "os.makedirs('frames', exist_ok=True)\n",
    "for i in range(n_frames):\n",
    "    angle = 2*math.pi * i / n_frames\n",
    "    mip = render_mip_axis(volume, angle, ROT_AXIS).cpu().numpy()\n",
    "    # Contrast\n",
    "    p1, p99 = np.percentile(mip, (1,99))\n",
    "    mip_norm = np.clip((mip-p1)/(p99-p1),0,1) if p99>p1 else (mip-mip.min())/(mip.max()-mip.min()+1e-6)\n",
    "    img = (mip_norm*255).astype('uint8')\n",
    "    Image.fromarray(img).resize((REC_SIZE,REC_SIZE)).save(f'frames/frame_{i:03d}.png')\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba51f739-e559-4db1-b33c-06049b20ada2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_260",
   "language": "python",
   "name": "pytorch_260"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
